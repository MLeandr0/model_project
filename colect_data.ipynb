{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_url = \"https://static.nhtsa.gov/odi/ffdd/cmpl/COMPLAINTS_RECEIVED_2015-2019.zip\"\n",
    "\n",
    "DOWNLOAD_DIR = \"downloads\"\n",
    "EXTRACT_DIR = \"extracted\"\n",
    "DATA_DIR = \"data\"\n",
    "DOWNLOADED_DATA = \"raw_complaints.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n",
    "os.makedirs(EXTRACT_DIR, exist_ok=True)\n",
    "os.makedirs(DATA_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(file_url):\n",
    "    file_name = file_url.split(\"/\")[-1]\n",
    "    file_path = os.path.join(DOWNLOAD_DIR, file_name)\n",
    "\n",
    "    print(f\"Downloading {file_name}...\")\n",
    "    response = requests.get(file_url)\n",
    "    if response.status_code == 200:\n",
    "        with open(file_path, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"Downloaded: {file_name}\")\n",
    "        return file_path\n",
    "    else:\n",
    "        print(f\"Failed to download {file_name}. Status: {response.status_code}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_file(file_path):\n",
    "    print(f\"Extracting {file_path}...\")\n",
    "    try:\n",
    "        with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(EXTRACT_DIR)\n",
    "        print(f\"Extraction completed.\")\n",
    "    except zipfile.BadZipFile as e:\n",
    "        print(f\"Failed to extract {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_txt_file(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File '{file_path}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(DATA_DIR):\n",
    "        os.makedirs(DATA_DIR)\n",
    "\n",
    "    output_path = os.path.join(DATA_DIR, DOWNLOADED_DATA)\n",
    "\n",
    "    print(f\"Processing {file_path}...\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, delimiter=\"\\t\", low_memory=False)\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"File converted and saved as {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_file = download_file(file_url)\n",
    "if downloaded_file:\n",
    "    extract_file(downloaded_file)\n",
    "    process_txt_file(downloaded_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add the missing header to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = [\n",
    "    \"CMPLID\", \"ODINO\", \"MFR_NAME\", \"MAKETXT\", \"MODELTXT\", \"YEARTXT\", \"CRASH\",\n",
    "    \"FAILDATE\", \"FIRE\", \"INJURED\", \"DEATHS\", \"COMPDESC\", \"CITY\", \"STATE\", \"VIN\",\n",
    "    \"DATEA\", \"LDATE\", \"MILES\", \"OCCURENCES\", \"CDESCR\", \"CMPL_TYPE\",\n",
    "    \"POLICE_RPT_YN\", \"PURCH_DT\", \"ORIG_OWNER_YN\", \"ANTI_BRAKES_YN\",\n",
    "    \"CRUISE_CONT_YN\", \"NUM_CYLS\", \"DRIVE_TRAIN\", \"FUEL_SYS\", \"FUEL_TYPE\",\n",
    "    \"TRANS_TYPE\", \"VEH_SPEED\", \"DOT\", \"TIRE_SIZE\", \"LOC_OF_TIRE\", \"TIRE_FAIL_TYPE\",\n",
    "    \"ORIG_EQUIP_YN\", \"MANUF_DT\", \"SEAT_TYPE\", \"RESTRAINT_TYPE\", \"DEALER_NAME\",\n",
    "    \"DEALER_TEL\", \"DEALER_CITY\", \"DEALER_STATE\", \"DEALER_ZIP\", \"PROD_TYPE\",\n",
    "    \"REPAIRED_YN\", \"MEDICAL_ATTN\", \"VEHICLES_TOWED_YN\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = \"data/raw_complaints.csv\"\n",
    "raw_data = pd.read_csv(raw_data_path, header=None, low_memory=False)\n",
    "\n",
    "raw_data.columns = header\n",
    "raw_data.columns = header\n",
    "\n",
    "output_file = \"data\\complaints_with_header.csv\"\n",
    "raw_data.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Header applied successfully. File saved as {output_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove unused columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"data/complaints_with_header.csv\"\n",
    "output_file = \"data/complains_treated.csv\"\n",
    "threshold = 0.8\n",
    "manual_columns_to_remove = [\"VIN\", \"PROD_TYPE\", \"POLICE_RPT_YN\", \"VEHICLES_TOWED_YN\", \"ANTI_BRAKES_YN\", \"CRUISE_CONT_YN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_columns = pd.read_csv(input_file, low_memory=False)\n",
    "\n",
    "non_null_percentage = df_with_columns.notnull().mean()\n",
    "columns_to_keep = non_null_percentage[non_null_percentage >= threshold].index\n",
    "columns_to_remove = set(manual_columns_to_remove)\n",
    "columns_to_keep = [col for col in columns_to_keep if col not in columns_to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_columns = df_with_columns[columns_to_keep]\n",
    "df_with_columns.to_csv(output_file, index=False)\n",
    "df_with_columns.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treat data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df = pd.read_csv(\"data/complains_treated.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, column_name, columns_to_keep_numbers):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    \n",
    "    # Step 1: Remove numbers and non-letter characters (for columns that do not need to keep numbers)\n",
    "    if column_name in columns_to_keep_numbers:\n",
    "        # Remove non-alphanumeric characters (but keep periods, numbers, and letters)\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s.]', '', text)\n",
    "    else:\n",
    "        # For other columns, remove everything except letters and spaces\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Convert to lowercase and clean up excess whitespace\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Step 2: Remove stopwords\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep_numbers = ['MODELTXT', 'MAKETXT', 'MFR_NAME', 'DATEA', 'LDATE']\n",
    "columns_to_encode = ['CRASH', 'FIRE', 'MEDICAL_ATTN', 'ORIG_OWNER_YN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "for col in columns_to_encode:\n",
    "    df[col] = label_encoder.fit_transform(df[col].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_columns = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "new_df = df.copy()\n",
    "\n",
    "for col in text_columns:\n",
    "    new_df[col] = new_df[col].apply(lambda x: preprocess_text(x, col, columns_to_keep_numbers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_columns = new_df.select_dtypes(include=['float']).columns\n",
    "for col in float_columns:\n",
    "    new_df[col] = new_df[col].apply(lambda x: int(x) if pd.notnull(x) else 0)  # Handle NaN values by converting to 0\n",
    "\n",
    "print(new_df.describe())\n",
    "print(new_df.info())\n",
    "\n",
    "new_df.to_csv(\"data/complains_preprocessed.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
